# Internal Lambda Troubleshooting Log Analysis Platform using Snowflake Cortex LLM

## Summary

This project demonstrates an internal, security-governed Lambda troubleshooting platform
that uses Snowflake Cortex LLM to analyze error logs and propose root causes and countermeasures
without exposing sensitive data to external AI services.

In enterprise environments, leveraging LLMs like ChatGPT or Claude for troubleshooting is highly convenient. However, Lambda error logs and incident logs often contain internal configurations, error details, and potentially sensitive personal information (PII). Using external LLMs requires sending internal logs to third-party AI companies, necessitating strict PII masking, log reduction, and security accountability, which increases operational costs significantly.

This project addresses these challenges by building a **self-contained troubleshooting log analysis platform** using Snowflake Cortex LLM, keeping all data within the company's security governance without exposing it externally.

---

## System Overview

### Background & Problem Recognition (Why)

Real-world challenges in troubleshooting:

- LLMs are extremely useful for troubleshooting
- But Lambda logs contain:
  - Internal configurations
  - Detailed error information
  - Potentially PII
- Issues with external LLM usage:
  - Need to send internal logs to external AI companies
  - Requires rigorous PII masking
  - Log content reduction
  - Security accountability
  - High operational costs

**Result:** "Convenient but impractical in practice"

### Why not external LLMs?

For example, a Lambda error log may contain:

- Internal resource names
- API endpoints
- User identifiers
- Stack traces revealing architecture details

Sending such logs to external LLM services introduces governance and compliance risks,
even if masking is applied.

### Solution Approach (What)

Concept: Build a troubleshooting log analysis platform using Snowflake Cortex LLM that processes internal data without external exposure.

Key Points:

- No use of external LLMs (ChatGPT, etc.)
- Complete within Snowflake's security governance
- Enables analysis without PII masking assumptions

**Result:** "Safe, easy, and context-preserving AI usage"

### Target Logs (Why this log)

Selected logs: AWS Lambda execution logs

Reasons:

- Automatically generated by CloudWatch Logs
- Contains all necessary elements for troubleshooting: errors, timeouts, latency
- Easy to intentionally cause failures for testing
- High frequency of practical use

**Approach:** Don't create apps for logs; let AWS generate logs automatically.

---

## Business Value

- Enables safe and efficient troubleshooting without external data exposure
- Reduces operational costs associated with PII masking and security measures
- Provides AI-powered log analysis within corporate governance
- Supports rapid incident response and system reliability improvement

---

## System Architecture

**AWS × Glue × Snowflake × Iceberg × Cortex LLM**

![Architecture Diagram](img/architecture.png)

### Data Flow

1. API Gateway → Lambda (intentionally generate errors/delays)
2. CloudWatch Logs → S3 Raw Bucket (JSON/gzip)
3. Glue → S3 Staging Bucket (Parquet)
4. Snowflake External Table → Snowflake Iceberg Table (Parquet)
5. Snowflake Cortex LLM → Natural language analysis of error causes and solutions

---

## Layer Design Philosophy (Critical)

### Raw Layer

- Store CloudWatch Logs as-is
- JSON/gzip format
- Immutable (premise for reprocessing)
- No Parquet conversion

### Staging Layer (Glue)

- Convert JSON to Parquet
- Light typing and normalization
- Date partitioning
- Not Iceberg

### Analytics Layer (Snowflake + Iceberg)

- Structured aggregation in Snowflake
- Parquet management as Iceberg Tables
- Schema evolution and analysis optimization

**Key Insight:** Parquet is the "result for analysis," not the raw data.

---

## Snowflake Cortex LLM Usage (Core Design)

Strategy:

- ❌ Don't throw raw logs directly to LLM
- ✅ Aggregate numerical data and trends with SQL first, then pass to LLM

What Cortex does:

- Summarize error trends
- Infer causes of occurrence
- Present prioritized action plans

Example prompt:
"Please infer the causes of Lambda errors that occurred during this period and suggest expected countermeasures."

### Example Output (Cortex LLM)

> "The spike in Lambda errors between 10:00–11:00 is likely caused by timeout misconfiguration.
> The average execution duration exceeded the configured timeout during this period.
> Consider increasing the timeout or optimizing the downstream API calls."

**Principle:** Numbers from SQL, meaning from LLM

---

## Security & Governance Perspective

Advantages of using Cortex LLM:

- Data never leaves the Snowflake account
- No log transmission to external AI companies
- AI usage under internal control

Accurate understanding:
"It's not 'no thinking required,' but operational load is significantly reduced compared to PII masking and log reduction assuming external transmission."

---

## Key Features & Technical Stack

### Key Features

- Self-contained log analysis without external data exposure
- Automated data pipeline from CloudWatch to Snowflake
- AI-powered error analysis and solution suggestions
- Layered data architecture for optimal processing
- Security-compliant AI integration

### Technical Stack

- **Cloud Platform:** AWS (Lambda, CloudWatch, S3, Glue)
- **Data Warehouse:** Snowflake
- **Table Format:** Apache Iceberg
- **AI/ML:** Snowflake Cortex LLM
- **Data Processing:** AWS Glue
- **Storage:** S3
- **API:** API Gateway

---

## Infrastructure as Code (IaC)

Infrastructure components are defined using Terraform for:

- Reproducible deployments
- Version-controlled configurations
- Scalable and maintainable architecture

Terraform configurations are organized in the `terraform/` directory.

---

## CI/CD Pipeline

Automated deployment and updates using GitHub Actions:

- Code linting and testing
- Infrastructure provisioning
- Data pipeline deployment
- Model and analysis updates

Workflow definitions are located in the `.github/workflows/` directory.

---

## Purpose

This project demonstrates the ability to design and implement **secure, AI-integrated data analysis systems** for enterprise environments.

This design is intended for enterprise environments where
security governance and data confidentiality are strict requirements.

It showcases skills in:

- Cloud-native data engineering
- AI integration within security boundaries
- Log analysis and troubleshooting systems
- Multi-layer data architecture design
- Security-conscious system development

All design decisions reflect real-world enterprise constraints and best practices.

---

## Key Takeaways

- Prioritizing data security and governance in AI implementations
- Strategic use of LLMs for analysis rather than raw processing
- Layered architecture for efficient data processing
- Business-focused design for operational troubleshooting
- Integration of modern data technologies (Iceberg, Snowflake) with AI
